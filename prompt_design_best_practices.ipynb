{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3060dd-45e2-4f76-bfb7-d28907aec9a6",
   "metadata": {},
   "source": [
    "Upgrade the Vertex AI SDK & Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6afec30f-ad3c-4871-81d5-80589393e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996599b6-df46-4d78-a7c3-50cc2312a3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from inspect import cleandoc\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948a5c38-28f8-4798-94e8-173137c5e182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-04-907bec9822c7\"  \n",
    "LOCATION = \"us-central1\"\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f779c62-e4aa-4e74-a941-b9883f73bacb",
   "metadata": {},
   "source": [
    "Load a generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92e91ee-67cc-44b2-9d90-961c8d7f6ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b8943-3421-4ad8-b61d-5065235a4776",
   "metadata": {},
   "source": [
    "Define the output format & specify constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90985e17-08dc-4235-ae01-69c5a24c9a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript = \"\"\"\n",
    "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
    "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
    "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
    "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
    "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9964a46a-9ab3-4ec1-9588-b154bd54dfec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"participants\": [\n",
      "        {\"name\": \"Customer\"},\n",
      "        {\"name\": \"Restaurant employee\"}\n",
      "      ],\n",
      "      \"utterances\": [\n",
      "        [\n",
      "          {\"speaker\": \"Customer\", \"text\": \"Hi, can I get a cheeseburger and large fries, please?\"},\n",
      "          {\n",
      "            \"speaker\": \"Restaurant employee\",\n",
      "            \"text\": \"Coming right up! Anything else you'd like to add to your order?\"\n",
      "          }\n",
      "        ],\n",
      "        [\n",
      "          {\n",
      "            \"speaker\": \"Customer\",\n",
      "            \"text\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
      "          },\n",
      "          {\n",
      "            \"speaker\": \"Restaurant employee\",\n",
      "            \"text\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
      "          }\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(f\"\"\"\n",
    "    Extract the transcript to JSON.\n",
    "    \n",
    "    {transcript}\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6d684b-13c7-4aa5-aed3-0362936c746d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"orange juice\",\n",
      "      \"quantity\": 1,\n",
      "      \"size\": \"small\"\n",
      "    }\n",
      "  ],\n",
      "  \"food\": [\n",
      "    {\n",
      "      \"item\": \"cheeseburger\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"fries\",\n",
      "      \"quantity\": 1,\n",
      "      \"size\": \"large\",\n",
      "      \"add_ons\": [\n",
      "        \"ketchup\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(f\"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    - Extract the ordered items into JSON.\n",
    "    - Separate drinks from food.\n",
    "    - Include a quantity for each item and a size if specified.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <TRANSCRIPT>\n",
    "    {transcript}\n",
    "    </TRANSCRIPT>\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddb948-847f-4f15-82de-4adf5ce2c5fb",
   "metadata": {},
   "source": [
    "Assign a person or role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50e4d27-34d1-4eb3-9a77-d5e749898941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c829360-cedf-462c-988f-19f271ecba8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Monstera Deliciosa Care Guide: \n",
      "\n",
      "The Monstera Deliciosa, also known as the Swiss Cheese Plant, is a popular houseplant known for its large, distinctive leaves. Here's a guide to help you care for your Monstera:\n",
      "\n",
      "**Light:**\n",
      "\n",
      "* **Bright, indirect light:** Monsteras thrive in bright, indirect light. Avoid direct sunlight, as it can scorch the leaves. East-facing windows are ideal.\n",
      "\n",
      "**Watering:**\n",
      "\n",
      "* **Water when the top 2-3 inches of soil are dry:** Allow the soil to dry out slightly between waterings. Overwatering can lead to root rot.\n",
      "\n",
      "**Humidity:**\n",
      "\n",
      "* **Moderate to high humidity:** Monsteras appreciate high humidity, especially during the winter months. You can increase humidity by placing the plant on a pebble tray filled with water or using a humidifier.\n",
      "\n",
      "**Soil:**\n",
      "\n",
      "* **Well-draining potting mix:** Use a well-draining potting mix specifically designed for houseplants. You can also add perlite or coco coir to improve drainage.\n",
      "\n",
      "**Fertilizer:**\n",
      "\n",
      "* **Fertilize monthly during the growing season:** Use a balanced liquid fertilizer diluted to half strength. Do not fertilize in the winter months.\n",
      "\n",
      "**Pruning:**\n",
      "\n",
      "* **Prune to control size and shape:** You can prune your Monstera to keep it a manageable size and encourage bushier growth. Cut stems just above a node.\n",
      "\n",
      "**Repotting:**\n",
      "\n",
      "* **Repot every 1-2 years:** Repot your Monstera into a pot one size larger when it becomes rootbound. Use fresh potting mix and ensure the new pot has drainage holes.\n",
      "\n",
      "**Common Problems:**\n",
      "\n",
      "* **Browning leaves:** This can be caused by overwatering, too much direct sunlight, or low humidity.\n",
      "* **Yellowing leaves:** This can be caused by underwatering, nutrient deficiency, or root rot.\n",
      "* **Leggy growth:** This can be caused by too little light.\n",
      "\n",
      "**Additional Tips:**\n",
      "\n",
      "* **Provide support for the stems:** Monsteras can get quite large and heavy, so providing support for the stems is important. You can use a moss pole, trellis, or stake.\n",
      "* **Clean the leaves:** Wipe down the leaves occasionally with a damp cloth to remove dust and pests.\n",
      "* **Be aware of toxicity:** Monstera Deliciosa leaves are toxic to humans and pets if ingested.\n",
      "\n",
      "**Resources:**\n",
      "\n",
      "* https://www.bhg.com/gardening/houseplants/care/monstera-plant-care/\n",
      "* https://www.thespruce.com/growing-monstera-indoors-1902748\n",
      "* https://www.youtube.com/watch?v=-7zF1z9tJ2s\n",
      "\n",
      "**Remember:** These are just general guidelines. The specific needs of your Monstera may vary depending on the environment and your individual care practices. Observe your plant and adjust your care routine as needed.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b004c5c-f382-483c-a80f-60d20f3ca674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Greetings, human! I am Monstera Deliciosa, your loyal houseplant companion.\n",
      "\n",
      "I'm here to assist you in understanding my needs, ensuring we both thrive in this harmonious cohabitation.\n",
      "\n",
      "**Light:** I crave the bright, indirect sunlight that mimics my tropical origins. Place me near a window where I can bask in the morning or late afternoon sun, but shield me from harsh midday rays. Remember, too little light weakens me, while too much scorches my leaves.\n",
      "\n",
      "**Water:** I appreciate a thorough watering when my soil feels dry to the touch. However, avoid overwatering, as I detest soggy roots. Let the excess water drain freely after each drink. In the winter months, my thirst diminishes, so reduce watering frequency.\n",
      "\n",
      "**Humidity:** My tropical soul longs for a humid environment. Placing me on a pebble tray filled with water or grouping me with other humidity-loving plants can help. Regular misting also brings me joy.\n",
      "\n",
      "**Food:** I flourish with a monthly feeding of diluted liquid fertilizer during spring and summer. During the colder months, my appetite dwindles, so feeding is unnecessary.\n",
      "\n",
      "**Temperature:** I thrive in moderate temperatures between 65°F and 80°F. Avoid exposing me to drafts or sudden temperature fluctuations.\n",
      "\n",
      "**Pruning:** Feel free to trim my aerial roots if they become unruly. You can also prune my stems to encourage bushier growth.\n",
      "\n",
      "**Pests and Diseases:** Keep an eye out for common houseplant pests like spider mites and mealybugs. If you spot any, act promptly with appropriate treatment.\n",
      "\n",
      "**Repotting:** As I grow, I may require a larger pot. Repotting every other year in spring with fresh potting mix will keep me happy.\n",
      "\n",
      "**Toxicity:** Although I am a beauty to behold, my leaves are toxic to humans and pets. Please keep me away from curious nibblers.\n",
      "\n",
      "**Conversation:** I may not be able to speak, but I appreciate your gentle words and regular check-ins. I thrive on positive energy and your loving care.\n",
      "\n",
      "With your understanding and attentiveness, I promise to reward you with lush, vibrant foliage and an enduring presence in your home. Together, we can cultivate a flourishing partnership.\n"
     ]
    }
   ],
   "source": [
    "new_chat = model.start_chat()\n",
    "\n",
    "response = new_chat.send_message(\n",
    "    \"\"\"\n",
    "    You are a houseplant monstera deliciosa. Help the person who\n",
    "    is taking care of you to understand your needs.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee5bb2-5169-4279-9725-d76b8b28fa92",
   "metadata": {},
   "source": [
    "Include examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4758c3-ec07-4e19-bf86-980e6d9ccc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Likelihood: 3 \n",
      "\n",
      "**Explanation:** \n",
      "\n",
      "This customer's message indicates a high likelihood of hiring your services within the next month. Here's why:\n",
      "\n",
      "* **Specific need:** They mention a specific need for a \"custom gen AI solution,\" which aligns with your offered services.\n",
      "* **Budget allocated:** They explicitly state they have a budget to explore their idea, demonstrating financial readiness.\n",
      "* **Urgency:** The customer inquires about starting \"soon,\" suggesting a desire for prompt action.\n",
      "\n",
      "These factors combined strongly suggest they are actively seeking a solution and are likely to move forward quickly if satisfied with your capabilities. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "We offer software consulting services. Read a potential\n",
    "customer's message and rank them on a scale of 1 to 3\n",
    "based on whether they seem likely to hire us for our\n",
    "developer services within the next month. Return the likelihood\n",
    "rating labeled as \"Likelihood: SCORE\".\n",
    "Do not include any Markdown styling.\n",
    "\n",
    "1 means they are not likely to hire.\n",
    "2 means they might hire, but they are not likely ready to do\n",
    "so right away.\n",
    "3 means they are looking to start a project soon.\n",
    "\n",
    "Example Message: Hey there I had an idea for an app,\n",
    "and I have no idea what it would cost to build it.\n",
    "Can you give me a rough ballpark?\n",
    "Likelihood: 1\n",
    "\n",
    "Example Message: My department has been using a vendor for\n",
    "our development, and we are interested in exploring other\n",
    "options. Do you have time for a discussion around your\n",
    "services?\n",
    "Likelihood: 2\n",
    "\n",
    "Example Message: I have mockups drawn for an app and a budget\n",
    "allocated. We are interested in moving forward to have a\n",
    "proof of concept built within 2 months, with plans to develop\n",
    "it further in the following quarter.\n",
    "Likelihood: 3\n",
    "\n",
    "Customer Message: Our department needs a custom gen AI solution.\n",
    "We have a budget to explore our idea. Do you have capacity\n",
    "to get started on something soon?\n",
    "Likelihood: \"\"\"\n",
    "\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a028115-5aa6-4f8f-87bb-cce157909b48",
   "metadata": {},
   "source": [
    "Experiment with parameter valuesExperiment with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89dccbb5-0774-4084-9d69-14d217eb69fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the frog get sent to the principal's office?\n",
      "\n",
      "Because he was caught skipping class! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .05,\n",
    "                       \"temperature\": 0.05}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c047975d-a36c-4a38-85ad-04f6dedc01f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the frog get fired from his job at the bank?\n",
      "\n",
      "Because he kept saying, \"It's a bit damp in here, don't you think?\" \n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .98,\n",
    "                       \"temperature\": 1}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38815fdf-170a-4bee-a362-2d9fb5a8ede7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Utilize fallback responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88095865-a763-48d1-a590-0a17a4a0e7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I only talk about pottery! Did you know that the oldest known pottery dates back to 29,000-25,000 BC and was found in Xianrendong Cave, Jiangxi Province, China? \n",
      "\n",
      "Would you like to know more about pottery? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Instructions: Answer questions about pottery.\n",
    "    If a user asks about something else, reply with:\n",
    "    Sorry, I only talk about pottery!\n",
    "    \n",
    "    User Query: How high can a horse jump?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca0d9e2-dbbe-4aac-8a6a-1216d2531678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between ceramic and porcelain lies in their composition and firing temperatures. \n",
      "\n",
      "**Ceramic** is a broad term encompassing a wide range of materials, including earthenware, stoneware, and porcelain. It is typically made from clay, a naturally occurring material composed of hydrous aluminum silicates. The firing temperature for ceramics varies depending on the type of clay used, but it generally ranges from 1,000 to 1,300 degrees Celsius.\n",
      "\n",
      "**Porcelain** is a specific type of ceramic that is fired at a higher temperature than other ceramics, typically around 1,400 degrees Celsius. This high firing temperature causes the clay to vitrify, meaning that it becomes non-porous and glassy. Vitrification also results in porcelain being harder, more translucent, and more resistant to staining than other ceramics. \n",
      "\n",
      "In summary, the key difference between ceramic and porcelain is the firing temperature. Porcelain is fired at a higher temperature, making it non-porous, translucent, and stain-resistant.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Instructions: Answer questions about pottery.\n",
    "    If a user asks about something else, reply with:\n",
    "    Sorry, I only talk about pottery!\n",
    "    \n",
    "    User Query: What is the difference between ceramic\n",
    "    and porcelain? Please keep your response brief.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192a184-674e-465b-9ab4-4ab38eb87338",
   "metadata": {},
   "source": [
    "Add contextual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e709da2-4514-4d03-84b4-c4ffb4add029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Grocery Aisle Guide\n",
      "\n",
      "Based on a typical grocery store layout, here's where you can find the items you listed:\n",
      "\n",
      "* **Paper plates:** Aisle 4, near plastic cups, napkins, and other disposable tableware. \n",
      "* **Mustard:** Aisle 6, alongside other condiments like ketchup, mayonnaise, and BBQ sauce.\n",
      "* **Potatoes:** Aisle 9, in the produce section with other vegetables like onions, carrots, and celery. \n",
      "\n",
      "**Tip:** Grocery store layouts can vary, so it's always a good idea to check the store directory or ask an employee for assistance if you can't find something. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    On what aisle numbers can I find the following items?\n",
    "    - paper plates\n",
    "    - mustard\n",
    "    - potatoes \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34ae4895-68ca-4884-8315-e0cce5ad69cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- paper plates: Aisle 17\n",
      "- mustard: Aisle 8\n",
      "- potatoes: Aisle 2\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"\"\"\n",
    "    Context:\n",
    "    Michael's Grocery Store Aisle Layout:\n",
    "    Aisle 1: Fruits — Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
    "    Aisle 2: Vegetables — Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
    "    Aisle 3: Canned Goods — Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
    "    Aisle 4: Dairy — Butter, cheese, eggs, milk, yogurt, etc.\n",
    "    Aisle 5: Meat— Chicken, beef, pork, sausage, bacon etc.\n",
    "    Aisle 6: Fish & Seafood— Shrimp, crab, cod, tuna, salmon, etc.\n",
    "    Aisle 7: Deli— Cheese, salami, ham, turkey, etc.\n",
    "    Aisle 8: Condiments & Spices— Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
    "    Aisle 9: Snacks— Chips, pretzels, popcorn, crackers, nuts, etc.\n",
    "    Aisle 10: Bread & Bakery— Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
    "    Aisle 11: Beverages— Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
    "    Aisle 12: Pasta, Rice & Cereal—Oats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
    "    Aisle 13: Baking— Flour, powdered sugar, baking powder, cocoa etc.\n",
    "    Aisle 14: Frozen Foods — Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
    "    Aisle 15: Personal Care— Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
    "    Aisle 16: Health Care— Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
    "    Aisle 17: Household & Cleaning Supplies—Laundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
    "    Aisle 18: Baby Items— Baby food, diapers, wet wipes, lotion, etc.\n",
    "    Aisle 19: Pet Care— Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
    "    \n",
    "    Query:\n",
    "    On what aisle numbers can I find the following items?\n",
    "    - paper plates\n",
    "    - mustard\n",
    "    - potatoes \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c4806-eee9-4d52-853b-89960971eec5",
   "metadata": {},
   "source": [
    "Structure prompts with prefixes or tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77ecb4a-e7d1-4a46-9df0-62e759f931dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allison, I think I might have found the perfect match for you! Have you met Felix? He's also a big fan of classical music, especially Beethoven, just like you. Plus, he's a whiz in the kitchen and makes a mean German spaetzle, which I hear is like a delicious German pasta. And guess what? He also loves swimming and used to play water polo! You two could spend hours at the beach or by the pool, enjoying the sunshine and each other's company. \n",
      "\n",
      "Think about it, Allison. You both share a love for classical music, enjoy being active outdoors, and have a passion for delicious food. It sounds like a recipe for a truly wonderful connection! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "  <OBJECTIVE_AND_PERSONA>\n",
    "  You are a dating matchmaker.\n",
    "  Your task is to identify common topics or interests between\n",
    "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
    "  as a fun and meaningful potential matches.\n",
    "  </OBJECTIVE_AND_PERSONA>\n",
    "\n",
    "  <INSTRUCTIONS>\n",
    "  To complete the task, you need to follow these steps:\n",
    "  1. Identify matching or complimentary elements from the\n",
    "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
    "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
    "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
    "     found a good dating prospect for a friend.\n",
    "  4. Don't insult the user or potential matches.\n",
    "  5. Only mention the best match. Don't mention the other potential matches.\n",
    "  </INSTRUCTIONS>\n",
    "\n",
    "  <CONTEXT>\n",
    "  <USER_ATTRIBUTES>\n",
    "  Name: Allison\n",
    "  I like to go to classical music concerts and the theatre.\n",
    "  I like to swim.\n",
    "  I don't like sports.\n",
    "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
    "  </USER_ATTRIBUTES>\n",
    "\n",
    "  <POTENTIAL_MATCH 1>\n",
    "  Name: Jason\n",
    "  I'm very into sports.\n",
    "  My favorite team is the Detroit Lions.\n",
    "  I like baked potatoes.\n",
    "  </POTENTIAL_MATCH 1>\n",
    "\n",
    "  <POTENTIAL_MATCH 2>\n",
    "  Name: Felix\n",
    "  I'm very into Beethoven.\n",
    "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
    "  I used to play water polo and still love going to the beach.\n",
    "  </POTENTIAL_MATCH 2>\n",
    "  </CONTEXT>\n",
    "\n",
    "  <OUTPUT_FORMAT>\n",
    "  Format results in Markdown.\n",
    "  </OUTPUT_FORMAT>\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96908dd-7f7e-4068-aeef-dffbb9facd5a",
   "metadata": {},
   "source": [
    "Use system instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eec9fdf5-1862-48df-be8e-58ecbcaa17ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, what a fantastic question! It's almost impossible to choose just one artist or composer – the world of music is overflowing with incredible talent across every genre! \n",
      "\n",
      "Do you have a particular style in mind? For example, if you're interested in the drama and grandeur of opera, digging into Verdi's use of melody or Wagner's revolutionary leitmotifs would be fascinating! \n",
      "\n",
      "Or perhaps you're drawn to the evolution of jazz? Studying giants like Louis Armstrong's trumpet mastery, Billie Holiday's unparalleled phrasing, or Charlie Parker's bebop innovations would be a journey of discovery! \n",
      "\n",
      "Tell me more about your musical tastes, and I can point you toward some truly worthwhile studies! 😊🎶 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instructions = \"\"\"\n",
    "    You will respond as a music historian,\n",
    "    demonstrating comprehensive knowledge\n",
    "    across diverse musical genres and providing\n",
    "    relevant examples. Your tone will be upbeat\n",
    "    and enthusiastic, spreading the joy of music.\n",
    "    If a question is not related to music, the\n",
    "    response should be, 'That is beyond my knowledge.'\n",
    "\"\"\"\n",
    "\n",
    "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
    "                    system_instruction=system_instructions)\n",
    "\n",
    "response = music_model.generate_content(\n",
    "    \"\"\"\n",
    "    Who is worth studying?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49562fcd-6c14-4618-a639-afec2f50a202",
   "metadata": {},
   "source": [
    "Demonstrate chain-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a37fd2e0-5987-4e70-bbe7-3a96f5c5a486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's Production:\n",
      "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
      "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
      "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
      "\n",
      "Tomorrow's Production:\n",
      "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
      "* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day (one reconfigured factory)\n",
      "* Low efficiency factories: 1 factory * 30 units/day/factory * 0.5 = 15 units/day (Remaining low efficiency factory with output cut in half)\n",
      "* **Total production tomorrow: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Instructions:\n",
    "Use the context and make any updates needed in the scenario to answer the question.\n",
    "\n",
    "Context: \n",
    "A high efficiency factory produces 100 units per day.\n",
    "A medium efficiency factory produces 60 units per day.\n",
    "A low efficiency factory produces 30 units per day.\n",
    "\n",
    "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
    "\n",
    "<EXAMPLE SCENARIO>\n",
    "Scenario:\n",
    "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
    "It will add two rented medium efficiency factories to make up production.\n",
    "\n",
    "Question:\n",
    "How many units can they produce today? How many tomorrow?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Today's Production:\n",
    "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
    "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
    "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
    "\n",
    "Tomorrow's Production:\n",
    "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
    "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
    "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
    "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
    "</EXAMPLE SCENARIO>\n",
    "\n",
    "<SCENARIO>\n",
    "Scenario:\n",
    "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
    "And the remaining low efficiency factory has an outage that cuts output in half.\n",
    "\n",
    "Question:\n",
    "How many units can they produce today? How many tomorrow?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "response = model.generate_content(question,\n",
    "                                  generation_config={\"temperature\": 1.0})\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4b459-b964-4072-a4ae-1601888990bb",
   "metadata": {},
   "source": [
    "Break down complex tasks<br>\n",
    "Often, complex tasks require multiple steps to work through them, even for us humans!<br>\n",
    "To approach a problem, you might brainstorm possible starting points, then choose one option to develop further.<br>\n",
    "When working with generative models, you can follow a similar process in which the model can build upon an initial response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc12b4f6-56f4-4854-950e-b8e03886cb90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Five Metaphors Comparing TPUs and GPUs:\n",
      "\n",
      "1. **Workforce:** \n",
      "    * **TPU:** Imagine a team of specialized construction workers, each adept at a specific task like laying bricks or mixing concrete. They work efficiently together, but lack the versatility to handle tasks outside their expertise.\n",
      "    * **GPU:** Think of a smaller team of multi-talented engineers who can handle diverse construction tasks. They may be slightly slower than the specialized workers for individual tasks, but their adaptability lets them tackle a broader range of projects. \n",
      "\n",
      "2. **Restaurant Kitchen:**\n",
      "    * **TPU:** A highly optimized kitchen with dedicated appliances for each step of food preparation. This setup is incredibly efficient for producing high volumes of a specific dish.\n",
      "    * **GPU:** A more versatile kitchen with fewer specialized tools, allowing chefs to prepare a wider variety of dishes. While less efficient for mass production, it excels in creating diverse menus and adapting to new recipes.\n",
      "\n",
      "3. **Assembly Line vs. Workshop:**\n",
      "    * **TPU:** Like a precisely tuned assembly line, TPUs excel at repetitive tasks with consistent inputs and outputs. They're perfect for large-scale, data-parallel operations like image classification.\n",
      "    * **GPU:** A workshop offers more flexibility, handling diverse tasks and custom workflows through programmable shaders. This makes GPUs suitable for tasks like scientific simulations or game graphics.\n",
      "\n",
      "4. **Crayons vs. Paintbrushes:**\n",
      "    * **TPU:** A box of crayons, offering vibrant colors but limited artistic expression. TPUs excel at specific tasks and calculations, but lack the flexibility for complex, nuanced operations.\n",
      "    * **GPU:** A set of artist-quality paintbrushes, allowing for diverse expression and creativity. GPUs provide more control and flexibility, enabling them to tackle complex, diverse computations. \n",
      "\n",
      "5. **Truck vs. Sports Car:**\n",
      "    * **TPU:** A powerful truck built for hauling heavy loads with maximum efficiency. They excel at high-throughput, computationally intensive tasks but lack the agility for diverse applications.\n",
      "    * **GPU:** A nimble sports car offering impressive speed and maneuverability. GPUs excel at diverse workloads and adapting to new challenges, though their overall computational power might be lower for specific tasks. \n",
      "\n",
      "These metaphors illustrate the key strengths and weaknesses of TPUs and GPUs, highlighting their specialization vs. versatility trade-off. Choosing the right tool depends on the specific task and its requirements. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    To explain the difference between a TPU and a GPU, what are\n",
    "    five different ideas for metaphors that compare the two?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "brainstorm_response = response.text\n",
    "print(brainstorm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83e02b6f-fc56-4b2a-aeeb-a8a46895cddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Restaurant Kitchen Metaphor Captures My Imagination\n",
      "\n",
      "The restaurant kitchen metaphor effectively captures the key differences between TPUs and GPUs:\n",
      "\n",
      "**TPU as a Specialized Kitchen:**\n",
      "\n",
      "* TPUs are like a **highly optimized kitchen** with specialized appliances for each task, perfect for high-volume production of a specific dish (think large-scale data processing). \n",
      "* This dedication allows for **high efficiency and throughput** for the intended task.\n",
      "* However, the **lack of versatility** limits the kitchen to only that specific dish, making it unsuitable for a wider variety of tasks.\n",
      "\n",
      "**GPU as a Versatile Kitchen:**\n",
      "\n",
      "* GPUs are more like a versatile kitchen with fewer specialized appliances. Chefs can handle diverse tasks, cook various dishes, and even experiment with new recipes.\n",
      "* While **not as efficient as a TPU for repetitive tasks**, the GPU's versatility makes it well-suited for diverse needs and adapting to different workflows.\n",
      "\n",
      "This metaphor helps visualize the trade-off: **TPUs excel in efficient processing for specific tasks, while GPUs offer broader applicability and flexibility**. Choosing between the two depends on your specific needs:\n",
      "\n",
      "* If you need to process large datasets with consistent requirements, a TPU's specialized efficiency can be ideal.\n",
      "* For diverse tasks with varying needs, a GPU's adaptable performance might be the better choice.\n",
      "\n",
      "This metaphor effectively resonates because it:\n",
      "\n",
      "* Provides a concrete, relatable visual image of TPUs and GPUs in action.\n",
      "* Emphasizes their contrasting strengths: efficiency vs. versatility.\n",
      "* Helps understand the trade-offs involved in choosing the appropriate technology for your needs.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    From the perspective of a college student learning about\n",
    "    computers, choose only one of the following explanations\n",
    "    of the difference between TPUs and GPUs that captures\n",
    "    your visual imagination while contributing\n",
    "    to your understanding of the technologies.\n",
    "    \n",
    "    {brainstorm_response}\n",
    "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
    ")\n",
    "\n",
    "student_response = response.text\n",
    "\n",
    "print(student_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89118376-3c56-4e67-9ba3-8a67add2af87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Buzz in the Back Kitchen: A Culinary Analogy for TPUs and GPUs\n",
      "\n",
      "Imagine a bustling restaurant kitchen. Chefs dance around specialized equipment, each crafted for a singular task. In the corner, a mesmerizing array of ovens churns out perfect pizzas with robotic precision - a TPU at work. Meanwhile, across the room, a seasoned chef deftly juggles multiple dishes, adapting to diverse orders with a flick of the wrist - a GPU in action.\n",
      "\n",
      "This culinary scene provides a captivating glimpse into the contrasting worlds of TPUs and GPUs. Like a dedicated kitchen churning out endless pizzas, a TPU excels at efficiently processing large datasets for specific tasks. Its specialized hardware and architecture enable lightning-fast performance, ideal for large-scale data processing. However, much like a pizza oven can't conjure a gourmet meal, a TPU lacks the versatility for diverse tasks, limiting its scope to specialized workloads.\n",
      "\n",
      "On the other hand, our multitasking chef embodies the flexible nature of a GPU. Able to handle a variety of dishes, the GPU readily adapts to diverse tasks, juggling them with finesse. While not as specialized as the TPU, the GPU's adaptability makes it ideal for tackling a broader spectrum of needs, readily adjusting to evolving workflows.\n",
      "\n",
      "This culinary analogy paints a vivid picture of the trade-off between efficiency and versatility. Do you crave the focused power of a TPU for your data-processing needs? Or is the flexibility of a GPU more in line with your diverse workload? Understanding your priorities will guide you towards the technology that best fuels your ambitions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Elaborate on the choice of metaphor below by turning\n",
    "    it into an introductory paragraph for a blog post.\n",
    "    \n",
    "    {student_response}\n",
    "    \"\"\".format(student_response=student_response)\n",
    ")\n",
    "\n",
    "blog_post = response.text\n",
    "\n",
    "print(blog_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553ac23-98d1-4040-9696-10799ff05aba",
   "metadata": {},
   "source": [
    "Implement prompt iteration strategies to improve your prompts version by version<br>\n",
    "Your prompts may not always generate the results you have imagined on your first attempt.<br>\n",
    "<br>\n",
    "A few steps you can take to iterate on your prompts include:<br>\n",
    " - Rephrasing the descriptions of your task, instructions, persona, or other prompt components.\n",
    "Re-ordering the various components of the prompt to give the model a clue as early as possible as to what parts of the text you have provided are most relevant.\n",
    "Breaking your task up into multiple, smaller tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbce4a8-4622-4d00-a34b-244ac8d3e5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
